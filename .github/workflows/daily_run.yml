name: Daily Subsidy Data Update

on:
  schedule:
    - cron: '0 0 * * *' # 日本時間 午前9時 (UTC 0:00)
  workflow_dispatch: # 手動実行用

jobs:
  update-data:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Google Auth
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT_EMAIL }}
          create_credentials_file: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # 将来のサポート終了を考慮し3.11へ昇格

      - name: Install dependencies
        run: pip install requests google-api-python-client oauth2client beautifulsoup4 lxml

      - name: Run Indexing API with Sitemap Analysis
        env:
          AIRTABLE_API_KEY: ${{ secrets.AIRTABLE_API_KEY }}
          AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
          # Softrの標準サイトマップURLを設定
          SITEMAP_URL: "https://subsidy-checker-2026.softr.app/sitemap.xml"
        run: |
          python -c "
          import os
          import datetime
          import sys
          import requests
          from bs4 import BeautifulSoup
          from google.oauth2 import service_account
          from googleapiclient.discovery import build

          def run_update():
              try:
                  print(f'[{datetime.datetime.now()}] Starting Indexing API process...')
                  
                  sitemap_url = os.environ.get('SITEMAP_URL')
                  res = requests.get(sitemap_url, timeout=30)
                  
                  # エラーチェック: サイトマップが存在しない場合のハンドリング
                  if res.status_code != 200:
                      raise ConnectionError(f'Sitemap not found at {sitemap_url}. Status code: {res.status_code}')
                  
                  soup = BeautifulSoup(res.content, 'xml')
                  urls = [loc.text for loc in soup.find_all('loc')][:100]
                  
                  if not urls:
                      print('Warning: No URLs found in sitemap.')
                      return

                  print(f'Detected {len(urls)} URLs from sitemap.')

                  creds_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')
                  scopes = ['https://www.googleapis.com/auth/indexing']
                  credentials = service_account.Credentials.from_service_account_file(creds_path, scopes=scopes)
                  service = build('indexing', 'v1', credentials=credentials)

                  for url in urls:
                      try:
                          body = {'url': url, 'type': 'URL_UPDATED'}
                          service.urlNotifications().publish(body=body).execute()
                          print(f'Success: {url}')
                      except Exception as e:
                          print(f'Failed to notify {url}: {e}')

                  print('Daily Indexing process completed successfully.')

              except Exception as e:
                  print(f'Fatal Error: {str(e)}')
                  sys.exit(1)

          run_update()
          "
